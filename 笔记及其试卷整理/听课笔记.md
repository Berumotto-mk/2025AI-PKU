# 人工智能引论 听课笔记

[Introduction to Artificial Intelligence - 2023 Spring - PKU](https://github.com/EmptyBlueBox/Introduction_to_Artificial_Intelligence-llb-2023Spring-PKU)

## 第一节课绪论（2025.02.17）

### 智能体

## 第二节课（2.20）

### ai 数学基础

（以数学基础为主）
目录（图片）
概率论基础
确定性现象和随机现象
概论论：揭示**随机现象统计规律**的数学学科

#### 1.1 样本空间和样本点

样本空间
样本点：样本空间的元素，随机事件E可能出现的结果
空集

#### 1.2 随机事件

随机事件：满足某些条件的样本点组成的集合，是样本空间的子集
以投硬币为例（）
两种特殊的随机事件 必然事件，不可能事件
随机事件的关系
包含关系 事件的并，交，対立事件
互斥事件，对立事件

#### 1.3 古典概型

古典概型：满足两个条件
1.有限样本空间
2.等可能性： 每个试验结果在一次试验中出现的可能性相等
古典概率模型：
不同关系的两个随机事件各自的概率的关系

#### 1.4 条件概率

背景：在事件b已经发生下，事件a发生的概率，记作P(A|B) = P(AB)/P(B)
注意P(AB) =P(A交B)
例题：外出下雨时间
乘法公式：
及其推广

#### 1.5 事件的独立性

相互独立：P(A|B) =P(A) 则称两事件相互独立 推广多个事件相互独立
两两独立：三个事件a,b,c是三个事件，每其中两个都满足相互独立，但三个事件和在一起三个事件不一定是相互独立
很多都是从两个事件推到三个到多个

#### 1.6 全概率公式

完备事件组 :
全概率公式 ：
为什么出现全概率公式，是**由因求果**

#### 1.7 贝叶斯公式

贝叶斯公式分母使用了全概率公式
`<font color="red">`

#### 1.8随机变量

`<font color="black">`
随机变量
分布函数

## 第三节课

### python 基础

#### Numpy库

## 4.搜索概览 2月24日

### 什么是搜索

1. 目标

2. 状态 

- 开始状态
- 目标状态
- 当前状态

3. 动作

4. 状态转移模型

5. 动作代价函数

### 怎样解决一个搜索问题

- 解：一条从初始状态到目标状态的路径
- 路径：动作序列
- 路径的**总代价**：各个动作代价的总和
- 最优解：所有解中路径代价最小的解

### 常见搜索策略

#### 广搜

未找到目标，将所有未检测过的直接子节点加入到队列

#### 深搜

将一个尚未检验过的直接子节点加到列表末尾，直到加入该

## 5.搜索：UCS和A*

### UCS(uniform cost search)一致代价搜索

- 一定保证最优
- 与



### 使用的数据类型：优先队列 priority queue

**标注**：属于数据结构内容

1.堆

- 堆

父节点>子节点

- 堆的实现：
- 什么样的才算堆
- 堆插入操作的实现
- 堆移除操作（以移除根节点为例）

### 启发算法

### 盲目搜索和知情搜索

- 启发是一个估计当前状态离目标状态还有多远

**经典启发**：曼哈顿距离：水平距离加上竖直距离，O式距离

### 贪心搜索

### A*搜索

- 结合了贪心算法和UCS算法的特点

	o UCS 向后看，根据**至今积累**的代价排序 g(n) o 贪心 向前看, 根据离目标还有多远的估计排序 h(n)

- f(n) = g(n) + h(n)

- A*搜索不一定正确，h(启发）不一定真实，只有可接受启发才是正确的

- 为什么A*是最优的

证明：

对于A是一个最优的目标节点，

B是一个次优的目标节点

h是可接受的启发函数

1. A的前序节点n比B先离开
2. A的前序节点比B先扩展（ppt中未证明）

### 搜索中树与图的对比





- 搜索中使用图可以避免重复检索节点

### 对启发更严格的要求：一致启发

- 核心：单边的启发值≤实际动作代价
- 一致性保证了一条路径上的f值永远不会减少
- 一致性保证了在图搜上也是最优的（**证明**）

### A*的最优性

- 可接受性启发保证了树搜索A*的一致性
- 一致性启发保证了图搜索A*的一致性



## 6.搜索：逻辑和CSP

### 约束满足问题（constraint satisfaction problem）csp

- n个变量
	- 状态：一些变量的赋值
	- 目标：所有变量都有赋值，并且满足约束
	- 动作：对一个还没有赋值的变量进行赋值
- 定义
	- 特殊的搜索问题
	- 状态由变量 x定义，取值来自一个**域**
	- 目标测试是一系列的**约束**明确一些变量可以赋值的组合

### 例子：地图着色问题

### csp的一些常见类别

- 离散变量
	- 有限域：
	- 无穷域
- 连续变量

### 约束的常见类别

- 变量的个数定义了约束类别

例子:

- N-queen
- 数独
- 现实问题

### 解决问题

- 广搜
- 深搜
- 真正可以解决csp问题：

### 回溯搜索（Backtracking Search）

- 改进1：一次值考虑一个未赋值的变量
	- 变量赋值的顺序不重要
- 改进2：每一步都判断约束满足的情况
	- 只考虑域前序赋值不矛盾的可能
- 回溯搜索：深搜+如上2项改进

![image-20250424005439749](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20250424005439749.png)

### 对回溯算法的改进

从两方面进行改进

- 顺序
	- 顺序：带来最少约束的赋值（least constraining value）LCV
	- 顺序：选择最多约束存在的变量
		- 也即：最小剩余价值
- 筛选
	- **提前检查**：去掉那些加到现有赋值中就会违反一些约束的值
	- 约束传递（constraint propagation）：从约束推理到约束
		- 一条边的一致性
		- CSP整体的一致性

### **（可满足性问题）SAT问题：**

给定的一个或者多个逻辑语句，是否存在某个世界使所有语句为真

- DPLL算法
- CDCL算法（考试中不会涉及）

### 逻辑：

- 常量:true,False
- 变量:
- 连接符:与，或，非
- 字符
- 语句
- 世界

### 逻辑的相互转换

- 摩根定律
- 分配律

例子：DNF to DNF

- 合取范式（CNF）
- 析取范式（DNF）

SAT上更高效的算法

DPLL

- 合取范式
- 一个子句的4中可能
	- 在深搜过程中一次设置每个变量的值
	- 子句为假的时候，回溯
- 单字符传递
- 布尔约束传递 BCP

### CDCL算法

- 隐含图：两种赋值方式

1. 手工赋值
2. 由BCP赋值

三种节点

1. 自行赋值的节点（蓝色）
2. BCP（黄色）
3. 矛盾（红色）

### 比较划分

SAT求解器





## 第九节课  机器学习和线性回归

* time:3月20日

### 机器学习

* 什么是学习
* 什么是机器学习
* 传统编程和机器学习的区别

  ![1742455371848](image/听课笔记/1742455371848.png "传统编程与机器学习的区别")
* 机器学习的日常应用

  * 搜索引擎
  * 垃圾邮件分类器
  * 推荐系统
  * 人脸识别
  * 机器翻译
  * 数字助理
  * 交通预测
  * 语音识别
* 机器学习前延应用

  * ALphago
  * 自动驾驶
  * 蛋白质预测

#### 机器学习的类型

* 三种类型的模型
  * 判别式模型(discriminative Models)（需要标签label）
  * 描述式模型(Descriptive Models) (不需要标记)无监督试学习
  * 生成式模型(Generative Models)(不需要标记)无监督试学习
* 两种settings
  * 监督式学习（supervised Learning）
  * 非监督式学习(Unsupervised Learning)
  * **强化学习**:比较特殊的学习的方式 reinforcement learning

本课程主要介绍判别式模型

#### 判别型模型

* 模型（Model）:包含参数的（parameters）的函数* 标签（Lable）:要预测的类别或数值

  ![1742784773251](image/听课笔记/1742784773251.png)

- 模型训练/学习（training /learning）:通过调整模型参数来拟合（fit)训练数据（trainnig data)

##### 两种类型：

1. 分类(classification)：标签是离散的
2. 回归（regression）：标签是连续的

(对一些问题是回归还是分类进行区分)

##### 模型训练过程（Training）

- 输入：x = 邮箱 ，
- 目标：$\text{输出: } y \in \{\text{正常邮件}, \text{垃圾邮件}\}$
- 目标：$\text{fucation}  f(x)=y$

![1742458595262](image/听课笔记/1742458595262.png)

#### 模型评估

* 训练错误(training error)：在训练集上的平均误差
  * **最小化**训练误差来训练模型
  * 对于分类问题中通常用错误率来衡量训练误差
* 测试误差（test error）:在测试集上的平均误差
  * 训练完成后，用来真正衡量模型在新数据上的好坏
  * 衡量模型的泛化（generalization）能力
  * 训练误差低不一定测试误差低——过拟合
* 训练集（training set）、测试集（test set）
  * 对全部的数据，按一定比例随机划分为训练集和测试集
  * 按照时间顺序排序，再将前90%划分为训练集后，后10%测试集
* 过拟合（overfitting）：测试误差远远的大于训练误差
* 欠拟合（underfitting）:训练完成后，训练误差任然很大

![1742785468563](image/听课笔记/1742785468563.png)

#### K近临

* 对于一个最简单的机器学习算法
  * 对于一个测试样本，用训练样本中距离它最近的k个样本中占多数的来预测样本
  * 该图中训练样本中蓝色占多数，所以判定为蓝色

![1742459571194](image/听课笔记/1742459571194.png)

* 优点：

  * 不需要训练
  * 只要一个距离函数即可，默认欧式距离

  $$
  \|x_1 - x_2\| = \sqrt{\sum_{j \in [d]} \left(x_1^{(j)} - x_2^{(j)}\right)^2}
  $$
* 缺点

  * f需要储存所有的训练样本
  * 在测试时需要计算测试样本到所有训练样本的距离
  * 有时候很难找到一个好的距离函数

##### 纸级灾难(curse of Dimensionality)

有研究怎样将高维降级为低维

* 距离在高维失去意义，不再能很好的衡量远近
* K-NN不适合高维空间

#### 非参数化模型 vs 参数化模型

* 非参数化模型：

  * 例子：knn
  * 模型不能被
    * 1.有限参数(parameter)定义，
    * 2.或不包含参数
  * 需要保留训练数据，以对测试样本做出预测
* 参数化模型

  * 例子：线性回归，神经网络
  * 训练包含课训练的参数，通过拟合训练数据来估计模型参数
  * 训练好模型参数后，可以丢弃训练参数，仅仅依靠模型参数去预测新样本

### 最简单的参数化模型——线性模型

* 线性问题（Liner Model）

  * 线性模型$f(x) = w^Tx +b$
  * 输入$x \in \mathbb{R}^d$
  * 参数$w \in R^d ,b\in R,$分别称为权重(weight)和偏置（bias）
  * 输出$f(x) \in R$
  * 为每一个特证维度学习一个权重，反映其重要性
* 线性回归（Liner Regression）

  * 使用线性模型做回归问题
  * 标签$y \in R$

线性回归例子

* 偏置的作用：提供一个基础的预测值,所有特征在其基础上加权累加

线性回归可视化两个例子

#### 线性回归训练

* 如何训练
  * 通过最小化损失函数
* **平方损失函数（squared loss）**

$$
L(f(x_i),y_i) = (f(x_i)-y_i)^2
$$

* 惩罚那些预测值（prediction）偏离真实值（groundtruth）太大的情况
* 也称做最小二乘法（least squares）
* 求解最优化问题
* 

解决数值问题带来的不稳定的问题

## 第10节课 逻辑回归、多分类与正则化

* 经验分险最小化框架（ERM）

### 二分类问题

#### 逻辑回归

#### 二分类（Binary Classification）

* 标签只有两种

  * $y \in {-1,1}$,-1为负类，1为正类
* 一般不直接让$f(x)\in R$去拟合$y \in {-1,1}$
* 可以采用sign()函数将实数输出转换为{-1，1}、
* 
* 使用什么损失函数

  * 最直接的目标，最小化分类错误数，使用零一损失函数（zero-one loss）
  * 但是，零一损失函数是阶跃函数，不可微且不连续，无法使用梯度最优化
  * 

#### 最大似然框架（Maximum Likehood）

* 使用最大似然估计来推导适合二分类问题的损失函数
* 最大似然估计（MLE）的原则( **Maximum Likelihood Estimation**)
  * 对机器学习，每个观测数据既是
* 通过最大化观测数据在给定概率下的似然
  * 如果训练样本相互独立(独立分布假设)，最大似然估计可写为
  * 
  * 但是，大量概率连乘容易造成数值超出计算精度
  * 解决办法为，最大化对数似然(log-likehood)
    


    $$

#### 逻辑回归的最大似然估计

* 首先对$p(y|x;\theta)$建模
  * 对于已有的线性模型$f(x) = w^Tx + b$,只需要吧它转化为正类的概率
  * 采用sigmoid函数$\sigma \colon (-\infty, +\infty) \rightarrow [0,1]$
  * $$
    \begin{align*}
    \sigma(x) &= \frac{1}{1 + e^{-x}} \\
    1 - \sigma(x) &= \frac{e^{-x}}{1 + e^{-x}} = \frac{1}{1 + e^{x}} = \sigma(-x)
    \end{align*}
    $$
  * 则$p(y=1|x;\theta) = p(y=1|x;w,b)=\sigma(f(x))=\sigma(yf(x))$
  * 自然的$p(y= -1|x;w,b) = 1 -\sigma*(f(x))=\sigma(-f(x))=\sigma(yf(x))$
  * 说明，不论y取1/-1都有$p(y|x;w,b)=\sigma(yf(x))$
* 对

### 多分类问题

#### 采用二分类的思路解决多分类

* 对k分类的问题训练k个二分类器
* 第k个2分类器将k类当做正类，其余类当做负类
* 

#### Softmax回归

* 专门解决多分类问题
* k分类问题$y \in {1,2,K} = [X],x\in [K]$
* 共同训练K个模型
* 将模型输出$f_k(x)$转换为取第k类的概率
  * 不能使用sigmoid函数，因为需满足$$
  * 解决办法：使用softmax函数，使概率归一化
  * $$


    $$

### 正则化(Regularization)

* 目标：避免过拟合
* 实际中我们一般在损失函数后加一项一起优化：

  



* $\lambda R(X)$称为正则化项，用于惩罚过于复杂的模型
  * $\lambda > 0$是个超参数，余弦指定，不随参数优化
* 为什么需要正则化


#### 带正则化的回归

#### 带


## 13计算机视觉：图像分类与卷积神经网络

### 图像分类

#### 基于卷积神经网络的图像分类算法

##### 手工特征

1. 颜色
2. 纹理
3. 词袋

##### 卷积神经网络（CNN）（Convonlutional Neural Network）

##### 卷积层 （Convolution Layers）

###### 补零（Zero padding）

- 对原特征图补零，保证卷积输出后的特征图大小不变
- 对于滤波器的大小$ F \times F $
$$
F \times F\\
padding \ with \ (F-1)/2 \\
就能保持原有特征图大小
$$

###### 多个滤波器

###### 卷积神经网络

- 卷积层堆叠，卷积层后连接激活函数

##### 批归一化层（batch normalization）（BN层）

$$
Input : x: N \times D\\
\mu_j = \sum_{i=1}^{N}x_{i,j} \ 各个通道（维度）的均值，形状为D (D维的向量)\\
\sigma_j^2 = \frac {1}{N}\sum_i^N(x_{i,j}-\mu_j)^2 \ 方差  \\
\hat{x}_{i,j} = \frac {x_{i,j}-\mu_j}{\sqrt{\sigma_j^2 + \xi}} \ 标准化（N \times D） \\
y_{i,j} = \gamma_j \hat{x}_{i,j} +\beta_j  \ 输出（N \times D） \\
可学习的权重和偏移量：\quad \gamma ,\beta :(D维的向量) \\
当\gamma =\sigma,\beta = \mu \ 的时候，输出等于输入

- 
$$

- 将各维度特征标准化（0均值 ，1方差）
- 大致将各特征统一到相同尺度

###### 训练阶段

###### 测试阶段

$$
\mu_j = 训练阶段特征图的方差 \\
\sigma^2_j = 训练阶段特征图的方差
$$





- BN层变成了线性操作

##### 池化层（pooling）

- 将一个大的层转化成小的层表示

###### 最大池化（MaxPooling）

- 选择滤波器中最大的元素

##### Dropout

- 目的：增强roburst,增加样本
- Dropout操作
	- 训练过程中每一次迭代以p的概率随机丢弃一些神经元
	- 保留的神经元除1-p保持期望不变
	- 测试过程中使用全部的神经元

##### 全连接层

- 将$H \times W \times D$(长宽高)$->$拉直为$HWD \times 1$



- 每个输出值都由全部输入值和全连接层的一行点乘计算得

## 14.计算机视觉-三维重建

### 从三维到二维—摄像机几何

#### 针孔模型

#### 摄像机几何

摄像成像涉及到在不同坐标系统之间的转换

1. 世界坐标系统 ：XYZ；
2. 相机坐标系 ：xyz,取相机的光轴为z轴
3. 成像平面坐标系统：$x'y'$
4. 图像坐标系统：$uv$


$$
R\begin{pmatrix} X \\ Y \\ Z\end{pmatrix} +T = \begin{pmatrix} x \\ y \\ z \end{pmatrix} \quad (a) \  (世界坐标系统到相机坐标系统：旋转加平移)\\
T = \begin{pmatrix} d_x\\d_y\\d_z \end{pmatrix}\\
\frac{1}{z} \begin{pmatrix} f&0&0 \\ 0& f&0\\0& 0& 1 \end{pmatrix} \begin{pmatrix} x\\y\\z \end{pmatrix} = \begin{pmatrix} x'\\y'\\1 \end{pmatrix} \quad (b)\ 小孔成像
相似三角形\\
\begin{pmatrix} x'\\y' \end{pmatrix} +\begin{pmatrix} c_u \\c_y \end{pmatrix} = \begin{pmatrix} u \\ v\end{pmatrix} \quad (c)成像平面转换 \ 平移坐标\\
引入齐次坐标后的表达
$$




### 从二维到三维—三维重建

- 三维重建（定义）：根据二维投影或影像描述物体在三维中的结构和形状的过程

#### 基于对应点重建

##### 基本原理：

$$
A_1=K_1[R_1 \quad T_1] \quad A_2 = K_2[R_2 \quad T_2] \quad A表示从世界坐标系到图像坐标系的变换 \\
对于世界坐标系的点（X,Y,Z）有\quad A_1\begin{pmatrix}X\\Y\\Z\\1\end{pmatrix} =w_1\begin{pmatrix}u_1\\v_1\\1 \end{pmatrix}\quad A_2\begin{pmatrix}X\\Y\\Z\\1\end{pmatrix} =w_2\begin{pmatrix}u_2\\v_2\\1\end{pmatrix}\\
有A_1和A_2已知，解方程可得XYZ和w_1,w_2
$$






#### 以SIFT为例的特征提取和匹配过程

- （Scale-invariant feature transform,SIFT）尺度不变特征变换

### 前沿研究

#### 难点

- 遮挡现象
- 纹理缺失
- 光照变化

#### 最新技术（基于神经网络的三维重建）

- 

## 15.自然语言处理(NLP)-句法分析和分词

- 自然语言处理(natural language processing,NLP)
- NLP的作用
- NLP的难点
- 句法
- 语义：语言包含的含义
	- 词汇的语义

### 上下文无关的文法（Context-Free Grammar）

- 文法：一组规则，定义了合法短语的树结构
- 语言（language）:遵循规则的句子集合
- 



### 句法分析（Parsing）

- 根据文法规则分析一串单词以获得其短语结构的过程

#### CYK算法（自底而上的图表句法分析算法）

- 乔姆斯基范式（CNF）



### n元模型（n-gram）

### (标记词)分词（Tokenization）

- 分词（Word Tokenization）
	- 将文本序列分割为独立的单词
- 句子切分（断句）（Sentence Tokenization）
	- 将文本序列分割为独立的句子
- 中文分词

